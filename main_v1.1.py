import os
import json
import requests

# --- Part 1: The "Thinker" - LLM for Reasoning ---

def get_intermediate_representation(context_dict, model_name="llama3"):
    """
    Step 1 of the hybrid model. The LLM analyzes the context and extracts a clear,
    structured 'Intermediate Representation' (IR) of the user's intent.
    """
    prompt = f"""
    You are a highly intelligent context analysis AI. Your only job is to analyze a user's raw context
    and distill it into a simple, structured JSON object representing their core intent and key entities.
    This is the "Intermediate Representation" (IR) that a specialized model will use.

    Respond ONLY with a single, clean JSON object.

    EXAMPLE 1:
    CONTEXT: {{ "pending_task": "what's the score of the Pak v India match? Saad is asking on WhatsApp", "user_profile": {{...}} }}
    YOUR PERFECT OUTPUT:
    {{
      "intent": "get_live_score_and_reply",
      "entities": {{
        "topic": "Pakistan vs India Cricket Match",
        "recipient_name": "Saad",
        "recipient_contact": "+923001234567",
        "score_source_app": "Cricinfo",
        "reply_app": "WhatsApp"
      }}
    }}
    
    EXAMPLE 2:
    CONTEXT: {{ "pending_task": "Plan a dinner for the project team for tonight.", "user_profile": {{...}} }}
    YOUR PERFECT OUTPUT:
    {{
        "intent": "plan_team_dinner",
        "entities": {{
            "team_channel": "#project-alpha",
            "communication_app": "Slack",
            "food_ordering_app": "FoodPanda",
            "cuisine_preference": "spicy desi food",
            "budget_per_person_rs": 2000
        }}
    }}
    ---
    
    Now, analyze the following context and generate the IR.
    CONTEXT: {json.dumps(context_dict)}
    """
    
    # This is a generic function to call the local Ollama LLM.
    url = "http://localhost:11434/api/generate"
    data = { "model": model_name, "prompt": prompt, "format": "json", "stream": False }
    
    try:
        response = requests.post(url, json=data, timeout=120)
        response.raise_for_status()
        result = response.json()
        return json.loads(result['response']), True
    except Exception as e:
        return {"error": str(e)}, False

# --- Part 2: The "Doer" - Your Custom Model with a Dynamic Fallback ---

def generate_generic_workflow_with_llm(intent, entities, model_name="llama3"):
    """
    This is the DYNAMIC FALLBACK. It's called when the "Doer" encounters an unknown intent.
    It uses the LLM in a constrained mode to generate a plausible workflow.
    """
    prompt = f"""
    You are an AI workflow generator. Your task is to take a user's intent and a list of entities
    and generate a logical 'workflow' array using a strict command schema.

    Your response MUST be a single, clean JSON object containing ONLY the 'workflow' array.

    --- COMMAND SCHEMA ---
    - Each object in the array must have 'command' and 'parameters' keys.
    - Valid Commands: ['OPEN_APP', 'SEND_MESSAGE', 'APP_ACTION', 'CREATE_EMAIL', 'SET_REMINDER', 'BOOK_RIDE']
    - `APP_ACTION` is for doing something inside an app (e.g., searching, playing media).
    - Use variables like '{{variable_name}}' for stateful actions.

    --- TASK ---
    Generate the 'workflow' array for the following user goal:
    - Intent: "{intent}"
    - Key Information (Entities): {json.dumps(entities)}
    """
    
    url = "http://localhost:11434/api/generate"
    data = { "model": model_name, "prompt": prompt, "format": "json", "stream": False }
    
    try:
        response = requests.post(url, json=data, timeout=120)
        response.raise_for_status()
        result = response.json()
        # The LLM should only return the workflow part. We wrap it in the full structure.
        workflow_data = json.loads(result['response'])
        return {
            "title": f"AI-Generated Plan for: {intent}",
            "body": "This workflow was dynamically generated by the AI to handle an unexpected task.",
            "workflow": workflow_data.get("workflow", []) # Safely get the workflow array
        }, True
    except Exception as e:
        return {"error": f"LLM fallback failed: {str(e)}"}, False

def generate_workflow_from_ir(intermediate_rep, model_name="llama3"):
    """
    The "Doer" model. It uses perfect, handcrafted workflows when possible,
    and falls back to the dynamic LLM generator for unknown intents.
    """
    intent = intermediate_rep.get("intent")
    entities = intermediate_rep.get("entities", {})

    if intent == "get_live_score_and_reply":
        # HANDCRAFTED WORKFLOW for perfect reliability
        return {
            "title": f"Check Score and Reply to {entities.get('recipient_name', 'Friend')}",
            "body": f"Finds the live cricket score on {entities.get('score_source_app', 'sports app')} and sends it to {entities.get('recipient_name', 'a friend')} via {entities.get('reply_app', 'chat')}.",
            "workflow": [
                {
                    "command": "APP_ACTION",
                    "parameters": {
                        "app_name": entities.get('score_source_app'),
                        "action_description": f"Search for live score of '{entities.get('topic')}'",
                        "output_variable": "live_score"
                    }
                },
                {
                    "command": "SEND_MESSAGE",
                    "parameters": {
                        "app": entities.get('reply_app'),
                        "to": entities.get('recipient_contact'),
                        "message": "Hey, the current score is: {live_score}"
                    }
                }
            ]
        }, True

    elif intent == "plan_team_dinner":
        # HANDCRAFTED WORKFLOW for perfect reliability
        return {
            "title": f"Plan Dinner for {entities.get('team_channel', 'the Team')}",
            "body": f"Coordinates with the team on {entities.get('communication_app', 'chat')} and finds suitable restaurants on {entities.get('food_ordering_app', 'the food app')}.",
            "workflow": [
                {
                    "command": "SEND_MESSAGE",
                    "parameters": {
                        "app": entities.get('communication_app'),
                        "to": entities.get('team_channel'),
                        "message": "Hey team, planning our dinner for tonight. Any specific restaurant requests for spicy desi food? Budget is around Rs. 2000 per person."
                    }
                },
                {
                    "command": "APP_ACTION",
                    "parameters": {
                        "app_name": entities.get('food_ordering_app'),
                        "action_description": f"Search for restaurants serving '{entities.get('cuisine_preference')}' with an average cost of 'Rs. {entities.get('budget_per_person_rs')}' per person",
                        "output_variable": "restaurant_options"
                    }
                }
            ]
        }, True

    else:
        # DYNAMIC FALLBACK for any other intent
        print("\n  > Custom handler not found for this intent. Engaging dynamic LLM fallback...")
        return generate_generic_workflow_with_llm(intent, entities, model_name)


# --- Main Execution ---
def run_hybrid_agent(context_dict, model_name="llama3"):
    """Executes the full Thinker -> Doer pipeline."""
    
    print("  AGENT STEP 1: Reasoning with LLM (The Thinker)...")
    intermediate_rep, success = get_intermediate_representation(context_dict, model_name)
    if not success:
        return intermediate_rep, False
    
    print("  > Intermediate Representation Generated:")
    print(json.dumps(intermediate_rep, indent=2))
    
    print("\n  AGENT STEP 2: Generating Workflow with Custom Model (The Doer)...")
    final_workflow, success = generate_workflow_from_ir(intermediate_rep, model_name)
    if success:
        print("  > Final Workflow Generated Successfully.")
    
    return final_workflow, success

if __name__ == '__main__':
    LOCAL_MODEL_NAME = "llama3"

    print("="*40)
    print(f"  Aura AI Hybrid Agent Engine")
    print("="*40 + "\n")

    CONTEXTS_DIR = "contexts"
    if not os.path.exists(CONTEXTS_DIR):
        os.makedirs(CONTEXTS_DIR)
        print(f"Directory '{CONTEXTS_DIR}' created. Please add context files.")
        exit()

    context_files = [f for f in os.listdir(CONTEXTS_DIR) if f.endswith('.json')]
    if not context_files:
        print(f"No JSON context files found in '{CONTEXTS_DIR}'.")
        exit()

    for file_name in context_files:
        file_path = os.path.join(CONTEXTS_DIR, file_name)
        print(f"--- Analyzing Scenario from: {file_name} ---")
        
        try:
            with open(file_path, 'r') as f:
                context_data = json.load(f)
            
            print(f"Pending Task: {context_data.get('pending_task', 'N/A')}\n")

            suggestion, success = run_hybrid_agent(context_data, LOCAL_MODEL_NAME)
            
            print("\n--- Final AI-Generated Workflow ---")
            if success:
                print(json.dumps(suggestion, indent=2))
            else:
                print(f"Error generating workflow: {suggestion}")

            print("\n" + "="*40 + "\n")

        except Exception as e:
            print(f"\nAn unexpected error occurred while processing '{file_name}': {e}")

